{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sundaybest3/s24Corpus-final/blob/main/Corpus/NOW_preprocess.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NOW data Text pre-processing\n",
        "\n",
        "+ Last updated (6/12)"
      ],
      "metadata": {
        "id": "sCLdjk-vtG1_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üçÄProcess:\n",
        "\n",
        "1. Downloaded NOW Sample Data from COCA\n",
        "\n",
        "*   The NOW corpus (News on the Web) sample data contains 1.7 million words of data from web-based newspapers and magazines from 2010 to 2016.\n",
        "*  While other resources like Google Trends show you what people are searching for, the NOW Corpus is the only structured corpus that shows you what is actually happening in the language -- virtually right up to the present time.\n",
        "\n",
        "2. Converted Txt to csv file.\n",
        "\n",
        "3. Removed \"@\", \"<p>\" and \"<h>\" characters.\n",
        "\n",
        "4. Split Text ID.\n",
        "\n"
      ],
      "metadata": {
        "id": "ykGQ_lCeumoU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/sundaybest3/s24Corpus-final/main/rawfile_now.txt'\n",
        "df = pd.read_csv(url, delimiter='\\t')  # Adjust delimiter as needed\n",
        "\n",
        "df.to_csv('rawfile_now.csv', index=False)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "uDokSgZYcFJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LInrQ2WgtDUY"
      },
      "outputs": [],
      "source": [
        "# Read the CSV file into a DataFrame\n",
        "df = pd.read_csv('rawfile_now.csv')\n",
        "\n",
        "df = df.replace('@', '', regex=True)\n",
        "df = df.replace(r'<\\/?p>|<\\/?h[0-9]?>', '', regex=True)\n",
        "\n",
        "print(df.head())\n",
        "\n",
        "df.to_csv('now_cleanfile.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üçÄTodo:"
      ],
      "metadata": {
        "id": "wSzsZ3XkvTCk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Step by step to get a cleaned text for the text column in our csv file\n",
        "\n",
        "+ Read csv file as data (using Github link)\n",
        "+ Read Column 'Text' and remove time stamps and parenthetical notes, and write the cleaned text in a new column named 'Cleantext01'"
      ],
      "metadata": {
        "id": "LrXT2uSfvsnr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split Text ID info\n"
      ],
      "metadata": {
        "id": "m5MpEKHgyI2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = 'now_cleanfile.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Split the 'textID' column into 'id' and 'text'\n",
        "data[['id', 'text']] = data['textID'].str.split(n=1, expand=True)\n",
        "\n",
        "# Drop the original 'textID' column\n",
        "data = data.drop(columns=['textID'])\n",
        "\n",
        "# Reorder the columns so that 'id' is the first column and 'text' is the second column\n",
        "data = data[['id', 'text'] + [col for col in data.columns if col not in ['id', 'text']]]\n",
        "\n",
        "# Display the cleaned data\n",
        "print(data.head())\n",
        "\n",
        "# Save the cleaned data to a new CSV file if needed\n",
        "data.to_csv('now_final.csv', index=False)"
      ],
      "metadata": {
        "id": "7D6e5RNMxjk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Find a word in the all text\n",
        "\n",
        "1. Combine the text and find a word\n",
        "2. For each text of the data ('Text'), find the word and add a new column with the number of cases found in the given text"
      ],
      "metadata": {
        "id": "IodeSSbP-OMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "LU0xshfF-z_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1a. Combine the text and find 'very' for example"
      ],
      "metadata": {
        "id": "vnvzHTVJA1w8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from io import StringIO\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')  # Tokenizer model\n",
        "\n",
        "# 1) Read a file from URL and assign the file to 'data' dataframe\n",
        "url = 'https://raw.githubusercontent.com/sundaybest3/s24Corpus-final/main/now_final.csv'  # Replace with your actual URL\n",
        "response = requests.get(url)\n",
        "data = pd.read_csv(StringIO(response.text))\n",
        "\n",
        "# 2) Display column names\n",
        "print(\"Column names:\", data.columns)\n",
        "print(\"=\"*50)\n",
        "# 3) Combine all items in the 'Text' column as a single string\n",
        "combined_text = ''.join(data['Text'].astype(str))\n",
        "\n",
        "# 4) Save the combined text as 'scriptall.txt'\n",
        "with open('/content/scriptall.txt', 'w', encoding='utf-8') as file:\n",
        "    file.write(combined_text)\n",
        "\n",
        "# 5) Remove punctuation using NLTK and save it as 'scriptall_nopunct.txt'\n",
        "tokens = word_tokenize(combined_text)\n",
        "tokens = [word for word in tokens if word.isalpha()]  # Remove punctuation\n",
        "text_no_punctuation = ' '.join(tokens)\n",
        "with open('/content/scriptall_nopunct.txt', 'w', encoding='utf-8') as file:\n",
        "    file.write(text_no_punctuation)\n",
        "\n",
        "# 6) Search matching strings 'very' (lower or capital) and display left and right 50 characters for all occurrences\n",
        "pattern = r'\\bvery\\b'  # Case-sensitive example; add (?i) for case-insensitive\n",
        "occurrences = 0\n",
        "for i, word in enumerate(tokens):\n",
        "    if word.lower() == 'very':\n",
        "        start = max(0, i - 10)  # Approximate word count before 'very'\n",
        "        end = min(len(tokens), i + 10)  # Approximate word count after 'very'\n",
        "        print(' '.join(tokens[start:end]))\n",
        "        occurrences += 1\n",
        "\n",
        "# 7) Print summary with how many occurrences are found in the given text\n",
        "print(\"=\"*50)\n",
        "print(f\"Total occurrences found: {occurrences}\")\n"
      ],
      "metadata": {
        "id": "oY1BcyD2-eDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1b. Combine the text and find a word (using user input)"
      ],
      "metadata": {
        "id": "FxNpW5XkAVW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from io import StringIO\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')  # Tokenizer model\n",
        "\n",
        "# 1) Read a file from URL and assign the file to 'data' dataframe\n",
        "url = 'https://raw.githubusercontent.com/MK316/Spring2024/main/Corpus/TEDdata/sample1.csv'\n",
        "response = requests.get(url)\n",
        "data = pd.read_csv(StringIO(response.text))\n",
        "\n",
        "# 2) Display column names\n",
        "print(\"Column names:\", data.columns)\n",
        "print(\"=\"*50)\n",
        "\n",
        "# 3) Combine all items in the 'Text' column as a single string\n",
        "combined_text = ''.join(data['Text'].astype(str))\n",
        "\n",
        "# 4) Save the combined text as 'scriptall.txt'\n",
        "with open('/content/scriptall.txt', 'w', encoding='utf-8') as file:\n",
        "    file.write(combined_text)\n",
        "\n",
        "# 5) Remove punctuation using NLTK and save it as 'scriptall_nopunct.txt'\n",
        "tokens = word_tokenize(combined_text)\n",
        "tokens = [word for word in tokens if word.isalpha()]  # Remove punctuation\n",
        "text_no_punctuation = ' '.join(tokens)\n",
        "with open('/content/scriptall_nopunct.txt', 'w', encoding='utf-8') as file:\n",
        "    file.write(text_no_punctuation)\n",
        "\n",
        "# Get user input for the word to find\n",
        "search_word = input(\"Enter the word to find: \")\n",
        "\n",
        "# 6) Search for the input word and display left and right 50 characters (approx. 10 words) for all occurrences\n",
        "occurrences = 0\n",
        "for i, word in enumerate(tokens):\n",
        "    if word.lower() == search_word.lower():\n",
        "        start = max(0, i - 10)  # Approximate word count before the search word\n",
        "        end = min(len(tokens), i + 10)  # Approximate word count after the search word\n",
        "        print(' '.join(tokens[start:end]))\n",
        "        occurrences += 1\n",
        "\n",
        "# 7) Print summary with how many occurrences are found in the given text\n",
        "print(\"=\"*50)\n",
        "print(f\"Total occurrences found: {occurrences}\")\n"
      ],
      "metadata": {
        "id": "s9UEW6ZWAYiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Find a word for each text and add the information as a separate column named 'CountVery'"
      ],
      "metadata": {
        "id": "wJJ9TCINAsWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown User input for a word to search, user input for the column name to record the number of occurrences\n",
        "import pandas as pd\n",
        "import requests\n",
        "from io import StringIO\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')  # Tokenizer model\n",
        "\n",
        "# 1) Read a file from URL and assign the file to 'data' dataframe\n",
        "url = 'https://raw.githubusercontent.com/MK316/Spring2024/main/Corpus/TEDdata/sample1.csv'\n",
        "response = requests.get(url)\n",
        "data = pd.read_csv(StringIO(response.text))\n",
        "\n",
        "# 2) Display column names\n",
        "print(\"Column names:\", data.columns)\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Get user input for the word to find and the new column name\n",
        "search_word = input(\"Enter the word to find: \").lower()\n",
        "new_column_name = input(\"Enter the new column name for word occurrences: \")\n",
        "\n",
        "# 3) Define a function to count occurrences of a specified word in a text\n",
        "def count_word_occurrences(text, word):\n",
        "    tokens = word_tokenize(text)\n",
        "    count = sum(1 for token in tokens if token.lower() == word)\n",
        "    return count\n",
        "\n",
        "# 4) Apply this function to each item in the 'Text' column and add the result to a new column\n",
        "data[new_column_name] = data['Text'].apply(lambda text: count_word_occurrences(text, search_word))\n",
        "\n",
        "# Display the updated DataFrame\n",
        "print(data[[new_column_name]].head())\n",
        "\n",
        "# Optionally, save the updated DataFrame to a new CSV file\n",
        "# data.to_csv('/content/updated_data.csv', index=False)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "x9bB_OAcArmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Final data to process**"
      ],
      "metadata": {
        "id": "EJMwHlbZtd8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [1] Data to read\n",
        "\n",
        "[data link](https://raw.githubusercontent.com/MK316/Spring2024/main/Corpus/TEDdata/TED100.csv)"
      ],
      "metadata": {
        "id": "rX9h9D6DtjHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "datalink = \"https://raw.githubusercontent.com/MK316/Spring2024/main/Corpus/TEDdata/TED100.csv\"\n",
        "data = pd.read_csv(datalink, encoding=\"utf-8\")\n",
        "data.head()"
      ],
      "metadata": {
        "id": "sSCI-U8EtiqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [2] Cleaned data: adding a column 'Cleanedtext01'\n",
        "\n",
        "+ data = original data\n",
        "+ df = cleaned data column added"
      ],
      "metadata": {
        "id": "5Ld-HeYEt264"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Assuming 'data' is your original DataFrame\n",
        "df = data\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove timestamps in the format \"00:00\"\n",
        "    text = re.sub(r'\\d{2}:\\d{2}\\n', '', text)\n",
        "    # Remove text within brackets\n",
        "    text = re.sub(r'\\(.*?\\)', '', text)\n",
        "    return text\n",
        "\n",
        "# Apply the clean_text function to each element in the 'Text' column\n",
        "df['Cleanedtext01'] = df['Text'].apply(clean_text)\n",
        "\n",
        "# Comparing the first item of 'Text' and 'Cleanedtext01'\n",
        "original_text = df['Text'].iloc[0][0:1000]  # Access the first item in the 'Text' column\n",
        "cleaned_text = df['Cleanedtext01'].iloc[0][0:1000]  # Access the first item in the 'Cleanedtext01' column\n",
        "\n",
        "print(\"Original Text:\")\n",
        "print(original_text)\n",
        "print(\"=\"*50)\n",
        "print(\"\\nCleaned Text:\")\n",
        "print(cleaned_text)"
      ],
      "metadata": {
        "id": "3hXxntMtt2FP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [3] Check whether the data cleaning is appropriately processed"
      ],
      "metadata": {
        "id": "TKwMLvBjuX-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 1. Check the first (timestamp) for both 'Text' and 'Cleanedtext01'\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Assuming 'df' is your DataFrame\n",
        "def remove_and_report_timestamps(text):\n",
        "    # Find all occurrences of the timestamp pattern\n",
        "    matches = re.findall(r'\\d{2}:\\d{2}\\n', text)\n",
        "    # Remove the timestamp pattern\n",
        "    cleaned_text = re.sub(r'\\d{2}:\\d{2}\\n', '', text)\n",
        "    return cleaned_text, matches\n",
        "\n",
        "# Apply the function and capture the cleaned text and the matches for 'Text'\n",
        "tn = input(\"Type the index of a text to check (1~100): \")\n",
        "tn = int(tn)\n",
        "n = tn-1\n",
        "cleaned_text_original, timestamp_matches_original = remove_and_report_timestamps(df['Text'][n])\n",
        "\n",
        "# Print the number of occurrences and list each occurrence for 'Text'\n",
        "if timestamp_matches_original:\n",
        "    print(f\"Found {len(timestamp_matches_original)} occurrences of the timestamp pattern in original text:\")\n",
        "    for match in timestamp_matches_original:\n",
        "        print(match.strip())  # .strip() is used to remove any trailing newline for clean display\n",
        "else:\n",
        "    print(\"No timestamp pattern found in the original text.\")\n",
        "\n",
        "# Apply the same function and capture the cleaned text and the matches for 'Cleanedtext01'\n",
        "cleaned_text_cleaned, timestamp_matches_cleaned = remove_and_report_timestamps(df['Cleanedtext01'][n])\n",
        "\n",
        "# Print the number of occurrences and list each occurrence for 'Cleanedtext01'\n",
        "if timestamp_matches_cleaned:\n",
        "    print(f\"Found {len(timestamp_matches_cleaned)} occurrences of the timestamp pattern in cleaned text:\")\n",
        "    for match in timestamp_matches_cleaned:\n",
        "        print(match.strip())\n",
        "else:\n",
        "    print(\"No timestamp pattern found in the cleaned text.\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "1b7_lZARuYHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 2. Check the second (parenthetical notes) for both 'Text' and 'Cleanedtext01'\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Assuming 'df' is your DataFrame\n",
        "def remove_and_report_timestamps(text):\n",
        "    # Find all occurrences of the timestamp pattern\n",
        "    matches = re.findall(r'\\(.*?\\)', text)\n",
        "    # Remove the timestamp pattern\n",
        "    cleaned_text = re.sub(r'\\(.*?\\)', '', text)\n",
        "    return cleaned_text, matches\n",
        "\n",
        "# Apply the function and capture the cleaned text and the matches for 'Text'\n",
        "ts = input(\"Which text to check (1~100): \")\n",
        "ts = int(ts)\n",
        "s = ts-1\n",
        "cleaned_text_original, timestamp_matches_original = remove_and_report_timestamps(df['Text'][s])\n",
        "\n",
        "# Print the number of occurrences and list each occurrence for 'Text'\n",
        "if timestamp_matches_original:\n",
        "    print(f\"Found {len(timestamp_matches_original)} occurrences of the timestamp pattern in original text:\")\n",
        "    for match in timestamp_matches_original:\n",
        "        print(match.strip())  # .strip() is used to remove any trailing newline for clean display\n",
        "else:\n",
        "    print(\"No timestamp pattern found in the original text.\")\n",
        "\n",
        "# Apply the same function and capture the cleaned text and the matches for 'Cleanedtext01'\n",
        "cleaned_text_cleaned, timestamp_matches_cleaned = remove_and_report_timestamps(df['Cleanedtext01'][s])\n",
        "\n",
        "# Print the number of occurrences and list each occurrence for 'Cleanedtext01'\n",
        "if timestamp_matches_cleaned:\n",
        "    print(f\"Found {len(timestamp_matches_cleaned)} occurrences of the timestamp pattern in cleaned text:\")\n",
        "    for match in timestamp_matches_cleaned:\n",
        "        print(match.strip())\n",
        "else:\n",
        "    print(\"No parenthetical pattern found in the cleaned text.\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "4ZYSbiLsuYHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [4] Text to combine for searching (to check)"
      ],
      "metadata": {
        "id": "yKL9NlRnwQvL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from io import StringIO\n",
        "import string\n",
        "\n",
        "\n",
        "# 2) Combine all items in the 'Text' column as a single string and remove punctuation\n",
        "combined_text = ''.join(df['Cleanedtext01'].astype(str))\n",
        "combined_text = combined_text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "# 3) Get user input for the word to find\n",
        "search_word = input(\"Enter the word to find: \")\n",
        "match_type = input(\"Type 'c' complete matches only, or 'p' for partial matches: \").lower()\n",
        "\n",
        "# 4) Function to find occurrences\n",
        "def find_occurrences(text, word, match_type):\n",
        "    occurrences = []\n",
        "    position = 0\n",
        "    while True:\n",
        "        if match_type == 'c':\n",
        "            # Find whole words only by using boundaries\n",
        "            position = text.lower().find(f' {word.lower()} ', position)\n",
        "        else:\n",
        "            position = text.lower().find(word.lower(), position)\n",
        "\n",
        "        if position == -1:  # No more occurrences found\n",
        "            break\n",
        "        # Calculate start and end positions for slicing\n",
        "        start = max(0, position - 30)\n",
        "        end = min(len(text), position + len(word) + 30)\n",
        "        occurrences.append(text[start:end])\n",
        "        position += len(word)  # Move past this occurrence\n",
        "\n",
        "    return occurrences\n",
        "\n",
        "occurrences = find_occurrences(combined_text, search_word, match_type)\n",
        "\n",
        "# 5) Decide how many occurrences to display\n",
        "print(f\"Total occurrences found: {len(occurrences)}\")\n",
        "print(\"=\"*50)\n",
        "if len(occurrences) > 10:\n",
        "    choice = input(\"More than 10 occurrences found. Type 'a' to display all or '10' to display only the first 10: \").lower()\n",
        "    print(\"=\"*50)\n",
        "    if choice == '10':\n",
        "        occurrences = occurrences[:10]\n",
        "\n",
        "# 6) Display occurrences\n",
        "for occurrence in occurrences:\n",
        "    print(occurrence)\n",
        "\n",
        "# 7) Print summary\n",
        "print(\"=\"*50)\n"
      ],
      "metadata": {
        "id": "ehy6FSAAxd0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving the processed file"
      ],
      "metadata": {
        "id": "sZJOJ_wH7Xm4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head())\n",
        "df.to_csv(\"Cleanedtext01.csv\", encoding = \"utf-8\", index=False)"
      ],
      "metadata": {
        "id": "9KqbYw4Y7cMJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}