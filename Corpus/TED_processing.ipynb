{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sundaybest3/s24Corpus-final/blob/main/Corpus/TED_processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Find a word in the TED text\n",
        "\n",
        "1. Combine the text and find a word\n",
        "2. For each text of the data ('Text'), find the word and add a new column with the number of cases found in the given text"
      ],
      "metadata": {
        "id": "IodeSSbP-OMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "LU0xshfF-z_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1a. Combine the text and find 'be+p.p.' for example\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vnvzHTVJA1w8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from io import StringIO\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re  # Ensure re is imported\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')  # Tokenizer model\n",
        "\n",
        "# 1) Read a file from URL and assign the file to 'data' dataframe\n",
        "url = 'https://raw.githubusercontent.com/sundaybest3/s24Corpus-final/main/Corpus/TEDdata/Cleantext0605.csv'\n",
        "response = requests.get(url)\n",
        "data = pd.read_csv(StringIO(response.text))\n",
        "\n",
        "# 2) Display column names\n",
        "print(\"Column names:\", data.columns)\n",
        "print(\"=\"*50)\n",
        "\n",
        "# 3) Combine all items in the 'text' column as a single string\n",
        "combined_text = ' '.join(data['Cleanedtext01'].astype(str))\n",
        "\n",
        "# 4) Save the combined text as 'scriptall.txt'\n",
        "with open('TEDscriptall.txt', 'w', encoding='utf-8') as file:\n",
        "    file.write(combined_text)\n",
        "\n",
        "# 5) Remove punctuation using NLTK and save it as 'scriptall_nopunct.txt'\n",
        "tokens = word_tokenize(combined_text)\n",
        "tokens = [word for word in tokens if word.isalpha()]  # Remove punctuation\n",
        "text_no_punctuation = ' '.join(tokens)\n",
        "with open('TEDscriptall_nopunct.txt', 'w', encoding='utf-8') as file:\n",
        "    file.write(text_no_punctuation)\n",
        "\n",
        "# 6) Search matching strings 'be + past participle' and display left and right 50 characters for all occurrences\n",
        "pattern = r'\\b(am|is|are|was|were|been|being)\\b\\s+\\b(\\w+ed|\\w+n)\\b'  # Regular expression for 'be + past participle'\n",
        "matches = []\n",
        "for i in range(len(tokens) - 1):\n",
        "    if re.match(r'\\b(am|is|are|was|were|been|being)\\b', tokens[i], re.IGNORECASE):\n",
        "        if re.match(r'\\b\\w+ed\\b|\\b\\w+n\\b', tokens[i + 1], re.IGNORECASE):\n",
        "            start = max(0, i - 10)  # Approximate word count before the match\n",
        "            end = min(len(tokens), i + 11)  # Approximate word count after the match\n",
        "            matches.append(' '.join(tokens[start:end]))\n",
        "\n",
        "# Print all matches\n",
        "for match in matches:\n",
        "    print(match)\n",
        "\n",
        "# 7) Print summary with how many occurrences are found in the given text\n",
        "print(\"=\"*50)\n",
        "print(f\"Total occurrences found: {len(matches)}\")"
      ],
      "metadata": {
        "id": "oY1BcyD2-eDn",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1b. Find 'be+p.p.+by+agent'"
      ],
      "metadata": {
        "id": "FxNpW5XkAVW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6) Search for 'be + past participle + by + agent' and display left and right 50 characters for all occurrences\n",
        "pattern = re.compile(r'\\b(am|is|are|was|were|been|being)\\b\\s+\\b(\\w+ed|\\w+n)\\b\\s+by\\s+\\b(\\w+)\\b', re.IGNORECASE)  # Regular expression for 'be + past participle + by + agent'\n",
        "matches = []\n",
        "for match in pattern.finditer(text_no_punctuation):\n",
        "    start = max(0, match.start() - 50)\n",
        "    end = min(len(text_no_punctuation), match.end() + 50)\n",
        "    matches.append(text_no_punctuation[start:end])\n",
        "\n",
        "# Print all matches\n",
        "for match in matches:\n",
        "    print(match)\n",
        "\n",
        "# 7) Print summary with how many occurrences are found in the given text\n",
        "print(\"=\"*50)\n",
        "print(f\"Total occurrences found: {len(matches)}\")\n"
      ],
      "metadata": {
        "id": "s9UEW6ZWAYiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eS1HaXGNmDbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Final data to process**"
      ],
      "metadata": {
        "id": "EJMwHlbZtd8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [2] Cleaned data: adding a column 'Cleanedtext01'\n",
        "\n",
        "+ data = original data\n",
        "+ df = cleaned data column added"
      ],
      "metadata": {
        "id": "5Ld-HeYEt264"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [3] Check whether the data cleaning is appropriately processed"
      ],
      "metadata": {
        "id": "TKwMLvBjuX-s"
      }
    }
  ]
}