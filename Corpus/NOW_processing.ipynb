{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sundaybest3/s24Corpus-final/blob/main/Corpus/NOW_processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NOW data Text pre-processing\n",
        "\n",
        "+ Last updated (6/12)"
      ],
      "metadata": {
        "id": "sCLdjk-vtG1_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üçÄProcess:\n",
        "\n",
        "1. Downloaded NOW Sample Data from COCA\n",
        "\n",
        "*   The NOW corpus (News on the Web) sample data contains 1.7 million words of data from web-based newspapers and magazines from 2010 to 2016.\n",
        "*  While other resources like Google Trends show you what people are searching for, the NOW Corpus is the only structured corpus that shows you what is actually happening in the language -- virtually right up to the present time.\n",
        "\n",
        "2. Converted Txt to csv file.\n",
        "\n",
        "3. Removed \"@\", \"<p>\" and \"<h>\" characters.\n",
        "\n",
        "4. Split Text ID.\n",
        "\n"
      ],
      "metadata": {
        "id": "ykGQ_lCeumoU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/sundaybest3/s24Corpus-final/main/rawfile_now.txt'\n",
        "df = pd.read_csv(url, delimiter='\\t')  # Adjust delimiter as needed\n",
        "\n",
        "df.to_csv('rawfile_now.csv', index=False)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "uDokSgZYcFJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LInrQ2WgtDUY"
      },
      "outputs": [],
      "source": [
        "# Read the CSV file into a DataFrame\n",
        "df = pd.read_csv('rawfile_now.csv')\n",
        "\n",
        "df = df.replace('@', '', regex=True)\n",
        "df = df.replace(r'<\\/?p>|<\\/?h[0-9]?>', '', regex=True)\n",
        "\n",
        "print(df.head())\n",
        "\n",
        "df.to_csv('now_cleanfile.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üçÄTodo:"
      ],
      "metadata": {
        "id": "wSzsZ3XkvTCk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Step by step to get a cleaned text for the text column in our csv file\n",
        "\n",
        "+ Read csv file as data (using Github link)\n",
        "+ Read Column 'Text' and remove time stamps and parenthetical notes, and write the cleaned text in a new column named 'Cleantext01'"
      ],
      "metadata": {
        "id": "LrXT2uSfvsnr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split Text ID info\n"
      ],
      "metadata": {
        "id": "m5MpEKHgyI2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = 'now_cleanfile.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Split the 'textID' column into 'id' and 'text'\n",
        "data[['id', 'text']] = data['textID'].str.split(n=1, expand=True)\n",
        "\n",
        "# Drop the original 'textID' column\n",
        "data = data.drop(columns=['textID'])\n",
        "\n",
        "# Reorder the columns so that 'id' is the first column and 'text' is the second column\n",
        "data = data[['id', 'text'] + [col for col in data.columns if col not in ['id', 'text']]]\n",
        "\n",
        "# Display the cleaned data\n",
        "print(data.head())\n",
        "\n",
        "# Save the cleaned data to a new CSV file if needed\n",
        "data.to_csv('now_final.csv', index=False)"
      ],
      "metadata": {
        "id": "7D6e5RNMxjk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Find a word in the all text\n",
        "\n",
        "1. Combine the text and find a word\n",
        "2. For each text of the data ('Text'), find the word and add a new column with the number of cases found in the given text"
      ],
      "metadata": {
        "id": "IodeSSbP-OMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "LU0xshfF-z_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1a. Combine the text and find 'be+p.p.' for example\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vnvzHTVJA1w8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from io import StringIO\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re  # Ensure re is imported\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')  # Tokenizer model\n",
        "\n",
        "# 1) Read a file from URL and assign the file to 'data' dataframe\n",
        "url = 'https://raw.githubusercontent.com/sundaybest3/s24Corpus-final/main/now_final.csv'  # Replace with your actual URL\n",
        "response = requests.get(url)\n",
        "data = pd.read_csv(StringIO(response.text))\n",
        "\n",
        "# 2) Display column names\n",
        "print(\"Column names:\", data.columns)\n",
        "print(\"=\"*50)\n",
        "\n",
        "# 3) Combine all items in the 'text' column as a single string\n",
        "combined_text = ' '.join(data['text'].astype(str))\n",
        "\n",
        "# 4) Save the combined text as 'scriptall.txt'\n",
        "with open('scriptall.txt', 'w', encoding='utf-8') as file:\n",
        "    file.write(combined_text)\n",
        "\n",
        "# 5) Remove punctuation using NLTK and save it as 'scriptall_nopunct.txt'\n",
        "tokens = word_tokenize(combined_text)\n",
        "tokens = [word for word in tokens if word.isalpha()]  # Remove punctuation\n",
        "text_no_punctuation = ' '.join(tokens)\n",
        "with open('scriptall_nopunct.txt', 'w', encoding='utf-8') as file:\n",
        "    file.write(text_no_punctuation)\n",
        "\n",
        "# 6) Search matching strings 'be + past participle' and display left and right 50 characters for all occurrences\n",
        "pattern = r'\\b(am|is|are|was|were|been|being)\\b\\s+\\b(\\w+ed|\\w+n)\\b'  # Regular expression for 'be + past participle'\n",
        "matches = []\n",
        "for i in range(len(tokens) - 1):\n",
        "    if re.match(r'\\b(am|is|are|was|were|been|being)\\b', tokens[i], re.IGNORECASE):\n",
        "        if re.match(r'\\b\\w+ed\\b|\\b\\w+n\\b', tokens[i + 1], re.IGNORECASE):\n",
        "            start = max(0, i - 10)  # Approximate word count before the match\n",
        "            end = min(len(tokens), i + 11)  # Approximate word count after the match\n",
        "            matches.append(' '.join(tokens[start:end]))\n",
        "\n",
        "# Print all matches\n",
        "for match in matches:\n",
        "    print(match)\n",
        "\n",
        "# 7) Print summary with how many occurrences are found in the given text\n",
        "print(\"=\"*50)\n",
        "print(f\"Total occurrences found: {len(matches)}\")"
      ],
      "metadata": {
        "id": "oY1BcyD2-eDn",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1b. Find 'be+p.p.+by+agent'"
      ],
      "metadata": {
        "id": "FxNpW5XkAVW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6) Search for 'be + past participle + by + agent' and display left and right 50 characters for all occurrences\n",
        "pattern = re.compile(r'\\b(am|is|are|was|were|been|being)\\b\\s+\\b(\\w+ed|\\w+n)\\b\\s+by\\s+\\b(\\w+)\\b', re.IGNORECASE)  # Regular expression for 'be + past participle + by + agent'\n",
        "matches = []\n",
        "for match in pattern.finditer(text_no_punctuation):\n",
        "    start = max(0, match.start() - 50)\n",
        "    end = min(len(text_no_punctuation), match.end() + 50)\n",
        "    matches.append(text_no_punctuation[start:end])\n",
        "\n",
        "# Print all matches\n",
        "for match in matches:\n",
        "    print(match)\n",
        "\n",
        "# 7) Print summary with how many occurrences are found in the given text\n",
        "print(\"=\"*50)\n",
        "print(f\"Total occurrences found: {len(matches)}\")\n"
      ],
      "metadata": {
        "id": "s9UEW6ZWAYiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eS1HaXGNmDbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Final data to process**"
      ],
      "metadata": {
        "id": "EJMwHlbZtd8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [2] Cleaned data: adding a column 'Cleanedtext01'\n",
        "\n",
        "+ data = original data\n",
        "+ df = cleaned data column added"
      ],
      "metadata": {
        "id": "5Ld-HeYEt264"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [3] Check whether the data cleaning is appropriately processed"
      ],
      "metadata": {
        "id": "TKwMLvBjuX-s"
      }
    }
  ]
}