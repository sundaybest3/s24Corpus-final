{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sundaybest3/s24Corpus-final/blob/main/Corpus/NOW_processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NOW data Text pre-processing\n",
        "\n",
        "+ Last updated (6/12)"
      ],
      "metadata": {
        "id": "sCLdjk-vtG1_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üçÄProcess:\n",
        "\n",
        "1. Downloaded NOW Sample Data from COCA\n",
        "\n",
        "*   The NOW corpus (News on the Web) sample data contains 1.7 million words of data from web-based newspapers and magazines from 2010 to 2016.\n",
        "*  While other resources like Google Trends show you what people are searching for, the NOW Corpus is the only structured corpus that shows you what is actually happening in the language -- virtually right up to the present time.\n",
        "\n",
        "2. Converted Txt to csv file.\n",
        "\n",
        "3. Removed \"@\", \"< p >\" and \"< h >\" characters.\n",
        "\n",
        "4. Split Text ID.\n",
        "\n"
      ],
      "metadata": {
        "id": "ykGQ_lCeumoU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/sundaybest3/s24Corpus-final/main/rawfile_now.txt'\n",
        "df = pd.read_csv(url, delimiter='\\t')  # Adjust delimiter as needed\n",
        "\n",
        "df.to_csv('rawfile_now.csv', index=False)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "uDokSgZYcFJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LInrQ2WgtDUY"
      },
      "outputs": [],
      "source": [
        "# Read the CSV file into a DataFrame\n",
        "df = pd.read_csv('rawfile_now.csv')\n",
        "\n",
        "df = df.replace('@', '', regex=True)\n",
        "df = df.replace(r'<\\/?p>|<\\/?h[0-9]?>', '', regex=True)\n",
        "\n",
        "print(df.head())\n",
        "\n",
        "df.to_csv('now_cleanfile.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üçÄTodo:"
      ],
      "metadata": {
        "id": "wSzsZ3XkvTCk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Step by step to get a cleaned text for the text column in our csv file\n",
        "\n",
        "+ Read csv file as data (using Github link)\n",
        "+ Read Column 'Text' and remove time stamps and parenthetical notes, and write the cleaned text in a new column named 'Cleantext01'"
      ],
      "metadata": {
        "id": "LrXT2uSfvsnr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split Text ID info\n"
      ],
      "metadata": {
        "id": "m5MpEKHgyI2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = 'now_cleanfile.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Split the 'textID' column into 'id' and 'text'\n",
        "data[['id', 'text']] = data['textID'].str.split(n=1, expand=True)\n",
        "\n",
        "# Drop the original 'textID' column\n",
        "data = data.drop(columns=['textID'])\n",
        "\n",
        "# Reorder the columns so that 'id' is the first column and 'text' is the second column\n",
        "data = data[['id', 'text'] + [col for col in data.columns if col not in ['id', 'text']]]\n",
        "\n",
        "# Display the cleaned data\n",
        "print(data.head())\n",
        "\n",
        "# Save the cleaned data to a new CSV file if needed\n",
        "data.to_csv('now_final.csv', index=False)"
      ],
      "metadata": {
        "id": "7D6e5RNMxjk4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}