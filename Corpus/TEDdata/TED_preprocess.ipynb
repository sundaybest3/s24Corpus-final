{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sundaybest3/s24Corpus-final/blob/main/Corpus/TEDdata/TED_preprocess.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TED data Text pre-processing\n",
        "\n",
        "+ Last updated (6/4)"
      ],
      "metadata": {
        "id": "sCLdjk-vtG1_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ€Todo: Removing timestamp and parenthetical notes in the script"
      ],
      "metadata": {
        "id": "ykGQ_lCeumoU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample input text\n",
        "text = \"\"\"\n",
        "The human voice: It's the instrument we all play. It's the most powerful sound in the world, probably. It's the only one that can start a war or say \"I love you.\" And yet many people have the experience that when they speak, people don't listen to them. And why is that? How can we speak powerfully to make change in the world?\n",
        "\n",
        "00:24\n",
        "What I'd like to suggest, there are a number of habits that we need to move away from. I've assembled for your pleasure here seven deadly sins of speaking. I'm not pretending this is an exhaustive list, but these seven, I think, are pretty large habits that we can all fall into.\n",
        "\n",
        "00:40\n",
        "First, gossip. Speaking ill of somebody who's not present. Not a nice habit, and we know perfectly well the person gossiping, five minutes later, will be gossiping about us.\n",
        "\n",
        "00:53\n",
        "Second, judging. We know people who are like this in conversation, and it's very hard to listen to somebody if you know that you're being judged and found wanting at the same time.\n",
        "\n",
        "01:03\n",
        "Third, negativity. You can fall into this. My mother, in the last years of her life, became very negative, and it's hard to listen. I remember one day, I said to her, \"It's October 1 today,\" and she said, \"I know, isn't it dreadful?\"\n",
        "\n",
        "01:16\n",
        "(Laughter)\n",
        "\n",
        "01:18\n",
        "It's hard to listen when somebody's that negative.\n",
        "\n",
        "01:21\n",
        "(Laughter)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "BZ3ocObktTYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ This script defines a function clean_text that takes in the raw text as input. It uses the re.sub function to substitute the patterns of timestamps and bracketed text with an empty string, effectively removing them from the text. After defining the function, you can pass any text to it to get the cleaned output.\n",
        "\n",
        "```\n",
        "re.sub(r'\\d{2}:\\d{2}\\n', '', text)\n",
        "```\n",
        "\n",
        "+ Pattern Explained:\n",
        "  + \\d{2}: This matches exactly two digits. The \\d denotes a digit (equivalent to [0-9]), and {2} specifies that exactly two instances of the preceding element (in this case, a digit) should be found.\n",
        "  + :: This matches the colon character exactly as it appears.\n",
        "  + \\d{2}: This again matches exactly two digits following the colon, corresponding to the minute format in a timestamp.\n",
        "  + \\n: This matches a newline character, ensuring that the pattern matches timestamps that are followed by a line break, which is typical when timestamps are used to label sections of text.\n",
        "+ Replacement: ' ' (an empty string) â€” this indicates that every match found by the pattern in the text should be replaced with nothing, effectively removing it.\n",
        "+ Application: This line of code will search through text for any sequence that matches a time stamp (e.g., \"01:21\") followed immediately by a newline, and removes those sequences."
      ],
      "metadata": {
        "id": "mfQhVPYxtbuA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LInrQ2WgtDUY"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove timestamps in the format \"00:00\"\n",
        "    text = re.sub(r'\\d{2}:\\d{2}\\n', '', text)\n",
        "    # Remove text within brackets\n",
        "    text = re.sub(r'\\(.*?\\)', '', text)\n",
        "    return text\n",
        "\n",
        "cleaned_text = clean_text(text)\n",
        "print(cleaned_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ€Todo:"
      ],
      "metadata": {
        "id": "wSzsZ3XkvTCk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Step by step to get a cleaned text for the text column in our csv file\n",
        "\n",
        "+ Read csv file as data (using Github link)\n",
        "+ Read Column 'Text' and remove time stamps and parenthetical notes, and write the cleaned text in a new column named 'Cleantext01'"
      ],
      "metadata": {
        "id": "LrXT2uSfvsnr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split Speaker info\n",
        "\n",
        "+ [data sample](https://raw.githubusercontent.com/MK316/Spring2024/main/Corpus/TEDdata/sample1.csv)"
      ],
      "metadata": {
        "id": "m5MpEKHgyI2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown Read csv, add Speaker information in 'Speaker' column\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Load the CSV file\n",
        "url = \"https://raw.githubusercontent.com/MK316/Spring2024/main/Corpus/TEDdata/sample1.csv\"\n",
        "# file_path = '/content/sample1.csv'  # Adjust the file path after uploading your file to Colab\n",
        "data = pd.read_csv(url)\n",
        "\n",
        "# Function to extract speaker name using regex\n",
        "def extract_speaker(title):\n",
        "    # This regex looks for the pattern where the name is in all capitals at the end of the string\n",
        "    match = re.search(r'\\n([A-Z\\s]+)$', title)\n",
        "    if match:\n",
        "        return match.group(1).strip()  # Return the matched group, stripped of leading/trailing whitespace\n",
        "    return ''  # Return empty string if no speaker name is found\n",
        "\n",
        "# Apply the function to separate the 'Speaker' from 'Title'\n",
        "data['Speaker'] = data['Title'].apply(extract_speaker)\n",
        "\n",
        "# Remove the speaker name from the 'Title' column\n",
        "data['Title'] = data['Title'].apply(lambda x: re.sub(r'\\n[A-Z\\s]+$', '', x))\n",
        "\n",
        "# Insert the 'Speaker' column right after the 'Title' column\n",
        "speaker_col = data.pop('Speaker')  # Remove the 'Speaker' column temporarily\n",
        "data.insert(1, 'Speaker', speaker_col)  # Insert it right after 'Title' column\n",
        "\n",
        "# Save the updated DataFrame to a new CSV file\n",
        "output_file_path = '/content/sample1_speaker.csv'\n",
        "data.to_csv(output_file_path, index=False)\n",
        "\n",
        "print(\"Updated DataFrame:\")\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "7D6e5RNMxjk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Clean-up [1]: removing timestamp and parenthetical notes, and add a new column\n",
        "\n",
        "```\n",
        "def clean_text(text):\n",
        "    # Remove timestamps in the format \"00:00\"\n",
        "    text = re.sub(r'\\d{2}:\\d{2}\\n', '', text)\n",
        "    # Remove text within brackets\n",
        "    text = re.sub(r'\\(.*?\\)', '', text)\n",
        "    return text\n",
        "```"
      ],
      "metadata": {
        "id": "LcGdTMZpy45c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data.head()\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Assuming 'data' is your original DataFrame\n",
        "df = data\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove timestamps in the format \"00:00\"\n",
        "    text = re.sub(r'\\d{2}:\\d{2}\\n', '', text)\n",
        "    # Remove text within brackets\n",
        "    text = re.sub(r'\\(.*?\\)', '', text)\n",
        "    return text\n",
        "\n",
        "# Apply the clean_text function to each element in the 'Text' column\n",
        "df['Cleanedtext01'] = df['Text'].apply(clean_text)\n",
        "\n",
        "# Comparing the first item of 'Text' and 'Cleanedtext01'\n",
        "original_text = df['Text'].iloc[0][0:1000]  # Access the first item in the 'Text' column\n",
        "cleaned_text = df['Cleanedtext01'].iloc[0][0:1000]  # Access the first item in the 'Cleanedtext01' column\n",
        "\n",
        "print(\"Original Text:\")\n",
        "print(original_text)\n",
        "print(\"=\"*50)\n",
        "print(\"\\nCleaned Text:\")\n",
        "print(cleaned_text)"
      ],
      "metadata": {
        "id": "Uj13_jpWy4ja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking whether the cleaning process is completed as planned\n",
        "\n",
        "1. find the timestamp expressions\n",
        "2. find parenthetical notes"
      ],
      "metadata": {
        "id": "9O-87Cka5nPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 1. Check the first (timestamp) for both 'Text' and 'Cleanedtext01'\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Assuming 'df' is your DataFrame\n",
        "def remove_and_report_timestamps(text):\n",
        "    # Find all occurrences of the timestamp pattern\n",
        "    matches = re.findall(r'\\d{2}:\\d{2}\\n', text)\n",
        "    # Remove the timestamp pattern\n",
        "    cleaned_text = re.sub(r'\\d{2}:\\d{2}\\n', '', text)\n",
        "    return cleaned_text, matches\n",
        "\n",
        "# Apply the function and capture the cleaned text and the matches for 'Text'\n",
        "cleaned_text_original, timestamp_matches_original = remove_and_report_timestamps(df['Text'][0])\n",
        "\n",
        "# Print the number of occurrences and list each occurrence for 'Text'\n",
        "if timestamp_matches_original:\n",
        "    print(f\"Found {len(timestamp_matches_original)} occurrences of the timestamp pattern in original text:\")\n",
        "    for match in timestamp_matches_original:\n",
        "        print(match.strip())  # .strip() is used to remove any trailing newline for clean display\n",
        "else:\n",
        "    print(\"No timestamp pattern found in the original text.\")\n",
        "\n",
        "# Apply the same function and capture the cleaned text and the matches for 'Cleanedtext01'\n",
        "cleaned_text_cleaned, timestamp_matches_cleaned = remove_and_report_timestamps(df['Cleanedtext01'][0])\n",
        "\n",
        "# Print the number of occurrences and list each occurrence for 'Cleanedtext01'\n",
        "if timestamp_matches_cleaned:\n",
        "    print(f\"Found {len(timestamp_matches_cleaned)} occurrences of the timestamp pattern in cleaned text:\")\n",
        "    for match in timestamp_matches_cleaned:\n",
        "        print(match.strip())\n",
        "else:\n",
        "    print(\"No timestamp pattern found in the cleaned text.\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "1dAB6oeP4GAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 2. Check the second (parenthetical notes) for both 'Text' and 'Cleanedtext01'\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Assuming 'df' is your DataFrame\n",
        "def remove_and_report_timestamps(text):\n",
        "    # Find all occurrences of the timestamp pattern\n",
        "    matches = re.findall(r'\\(.*?\\)', text)\n",
        "    # Remove the timestamp pattern\n",
        "    cleaned_text = re.sub(r'\\(.*?\\)', '', text)\n",
        "    return cleaned_text, matches\n",
        "\n",
        "# Apply the function and capture the cleaned text and the matches for 'Text'\n",
        "cleaned_text_original, timestamp_matches_original = remove_and_report_timestamps(df['Text'][0])\n",
        "\n",
        "# Print the number of occurrences and list each occurrence for 'Text'\n",
        "if timestamp_matches_original:\n",
        "    print(f\"Found {len(timestamp_matches_original)} occurrences of the timestamp pattern in original text:\")\n",
        "    for match in timestamp_matches_original:\n",
        "        print(match.strip())  # .strip() is used to remove any trailing newline for clean display\n",
        "else:\n",
        "    print(\"No timestamp pattern found in the original text.\")\n",
        "\n",
        "# Apply the same function and capture the cleaned text and the matches for 'Cleanedtext01'\n",
        "cleaned_text_cleaned, timestamp_matches_cleaned = remove_and_report_timestamps(df['Cleanedtext01'][0])\n",
        "\n",
        "# Print the number of occurrences and list each occurrence for 'Cleanedtext01'\n",
        "if timestamp_matches_cleaned:\n",
        "    print(f\"Found {len(timestamp_matches_cleaned)} occurrences of the timestamp pattern in cleaned text:\")\n",
        "    for match in timestamp_matches_cleaned:\n",
        "        print(match.strip())\n",
        "else:\n",
        "    print(\"No parenthetical pattern found in the cleaned text.\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Nb6IvDXM64x8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Find a word in the all text\n",
        "\n",
        "1. Combine the text and find a word\n",
        "2. For each text of the data ('Text'), find the word and add a new column with the number of cases found in the given text"
      ],
      "metadata": {
        "id": "IodeSSbP-OMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "LU0xshfF-z_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1a. Combine the text and find 'very' for example"
      ],
      "metadata": {
        "id": "vnvzHTVJA1w8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from io import StringIO\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')  # Tokenizer model\n",
        "\n",
        "# 1) Read a file from URL and assign the file to 'data' dataframe\n",
        "url = 'https://raw.githubusercontent.com/MK316/Spring2024/main/Corpus/TEDdata/sample1.csv'  # Replace with your actual URL\n",
        "response = requests.get(url)\n",
        "data = pd.read_csv(StringIO(response.text))\n",
        "\n",
        "# 2) Display column names\n",
        "print(\"Column names:\", data.columns)\n",
        "print(\"=\"*50)\n",
        "# 3) Combine all items in the 'Text' column as a single string\n",
        "combined_text = ''.join(data['Text'].astype(str))\n",
        "\n",
        "# 4) Save the combined text as 'scriptall.txt'\n",
        "with open('/content/scriptall.txt', 'w', encoding='utf-8') as file:\n",
        "    file.write(combined_text)\n",
        "\n",
        "# 5) Remove punctuation using NLTK and save it as 'scriptall_nopunct.txt'\n",
        "tokens = word_tokenize(combined_text)\n",
        "tokens = [word for word in tokens if word.isalpha()]  # Remove punctuation\n",
        "text_no_punctuation = ' '.join(tokens)\n",
        "with open('/content/scriptall_nopunct.txt', 'w', encoding='utf-8') as file:\n",
        "    file.write(text_no_punctuation)\n",
        "\n",
        "# 6) Search matching strings 'very' (lower or capital) and display left and right 50 characters for all occurrences\n",
        "pattern = r'\\bvery\\b'  # Case-sensitive example; add (?i) for case-insensitive\n",
        "occurrences = 0\n",
        "for i, word in enumerate(tokens):\n",
        "    if word.lower() == 'very':\n",
        "        start = max(0, i - 10)  # Approximate word count before 'very'\n",
        "        end = min(len(tokens), i + 10)  # Approximate word count after 'very'\n",
        "        print(' '.join(tokens[start:end]))\n",
        "        occurrences += 1\n",
        "\n",
        "# 7) Print summary with how many occurrences are found in the given text\n",
        "print(\"=\"*50)\n",
        "print(f\"Total occurrences found: {occurrences}\")\n"
      ],
      "metadata": {
        "id": "oY1BcyD2-eDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1b. Combine the text and find a word (using user input)"
      ],
      "metadata": {
        "id": "FxNpW5XkAVW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from io import StringIO\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')  # Tokenizer model\n",
        "\n",
        "# 1) Read a file from URL and assign the file to 'data' dataframe\n",
        "url = 'https://raw.githubusercontent.com/MK316/Spring2024/main/Corpus/TEDdata/sample1.csv'\n",
        "response = requests.get(url)\n",
        "data = pd.read_csv(StringIO(response.text))\n",
        "\n",
        "# 2) Display column names\n",
        "print(\"Column names:\", data.columns)\n",
        "print(\"=\"*50)\n",
        "\n",
        "# 3) Combine all items in the 'Text' column as a single string\n",
        "combined_text = ''.join(data['Text'].astype(str))\n",
        "\n",
        "# 4) Save the combined text as 'scriptall.txt'\n",
        "with open('/content/scriptall.txt', 'w', encoding='utf-8') as file:\n",
        "    file.write(combined_text)\n",
        "\n",
        "# 5) Remove punctuation using NLTK and save it as 'scriptall_nopunct.txt'\n",
        "tokens = word_tokenize(combined_text)\n",
        "tokens = [word for word in tokens if word.isalpha()]  # Remove punctuation\n",
        "text_no_punctuation = ' '.join(tokens)\n",
        "with open('/content/scriptall_nopunct.txt', 'w', encoding='utf-8') as file:\n",
        "    file.write(text_no_punctuation)\n",
        "\n",
        "# Get user input for the word to find\n",
        "search_word = input(\"Enter the word to find: \")\n",
        "\n",
        "# 6) Search for the input word and display left and right 50 characters (approx. 10 words) for all occurrences\n",
        "occurrences = 0\n",
        "for i, word in enumerate(tokens):\n",
        "    if word.lower() == search_word.lower():\n",
        "        start = max(0, i - 10)  # Approximate word count before the search word\n",
        "        end = min(len(tokens), i + 10)  # Approximate word count after the search word\n",
        "        print(' '.join(tokens[start:end]))\n",
        "        occurrences += 1\n",
        "\n",
        "# 7) Print summary with how many occurrences are found in the given text\n",
        "print(\"=\"*50)\n",
        "print(f\"Total occurrences found: {occurrences}\")\n"
      ],
      "metadata": {
        "id": "s9UEW6ZWAYiW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Find a word for each text and add the information as a separate column named 'CountVery'"
      ],
      "metadata": {
        "id": "wJJ9TCINAsWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown User input for a word to search, user input for the column name to record the number of occurrences\n",
        "import pandas as pd\n",
        "import requests\n",
        "from io import StringIO\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')  # Tokenizer model\n",
        "\n",
        "# 1) Read a file from URL and assign the file to 'data' dataframe\n",
        "url = 'https://raw.githubusercontent.com/MK316/Spring2024/main/Corpus/TEDdata/sample1.csv'\n",
        "response = requests.get(url)\n",
        "data = pd.read_csv(StringIO(response.text))\n",
        "\n",
        "# 2) Display column names\n",
        "print(\"Column names:\", data.columns)\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Get user input for the word to find and the new column name\n",
        "search_word = input(\"Enter the word to find: \").lower()\n",
        "new_column_name = input(\"Enter the new column name for word occurrences: \")\n",
        "\n",
        "# 3) Define a function to count occurrences of a specified word in a text\n",
        "def count_word_occurrences(text, word):\n",
        "    tokens = word_tokenize(text)\n",
        "    count = sum(1 for token in tokens if token.lower() == word)\n",
        "    return count\n",
        "\n",
        "# 4) Apply this function to each item in the 'Text' column and add the result to a new column\n",
        "data[new_column_name] = data['Text'].apply(lambda text: count_word_occurrences(text, search_word))\n",
        "\n",
        "# Display the updated DataFrame\n",
        "print(data[[new_column_name]].head())\n",
        "\n",
        "# Optionally, save the updated DataFrame to a new CSV file\n",
        "# data.to_csv('/content/updated_data.csv', index=False)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "x9bB_OAcArmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Final data to process**"
      ],
      "metadata": {
        "id": "EJMwHlbZtd8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [1] Data to read\n",
        "\n",
        "[data link](https://raw.githubusercontent.com/MK316/Spring2024/main/Corpus/TEDdata/TED100.csv)"
      ],
      "metadata": {
        "id": "rX9h9D6DtjHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "datalink = \"https://raw.githubusercontent.com/MK316/Spring2024/main/Corpus/TEDdata/TED100.csv\"\n",
        "data = pd.read_csv(datalink, encoding=\"utf-8\")\n",
        "data.head()"
      ],
      "metadata": {
        "id": "sSCI-U8EtiqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [2] Cleaned data: adding a column 'Cleanedtext01'\n",
        "\n",
        "+ data = original data\n",
        "+ df = cleaned data column added"
      ],
      "metadata": {
        "id": "5Ld-HeYEt264"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Assuming 'data' is your original DataFrame\n",
        "df = data\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove timestamps in the format \"00:00\"\n",
        "    text = re.sub(r'\\d{2}:\\d{2}\\n', '', text)\n",
        "    # Remove text within brackets\n",
        "    text = re.sub(r'\\(.*?\\)', '', text)\n",
        "    return text\n",
        "\n",
        "# Apply the clean_text function to each element in the 'Text' column\n",
        "df['Cleanedtext01'] = df['Text'].apply(clean_text)\n",
        "\n",
        "# Comparing the first item of 'Text' and 'Cleanedtext01'\n",
        "original_text = df['Text'].iloc[0][0:1000]  # Access the first item in the 'Text' column\n",
        "cleaned_text = df['Cleanedtext01'].iloc[0][0:1000]  # Access the first item in the 'Cleanedtext01' column\n",
        "\n",
        "print(\"Original Text:\")\n",
        "print(original_text)\n",
        "print(\"=\"*50)\n",
        "print(\"\\nCleaned Text:\")\n",
        "print(cleaned_text)"
      ],
      "metadata": {
        "id": "3hXxntMtt2FP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [3] Check whether the data cleaning is appropriately processed"
      ],
      "metadata": {
        "id": "TKwMLvBjuX-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 1. Check the first (timestamp) for both 'Text' and 'Cleanedtext01'\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Assuming 'df' is your DataFrame\n",
        "def remove_and_report_timestamps(text):\n",
        "    # Find all occurrences of the timestamp pattern\n",
        "    matches = re.findall(r'\\d{2}:\\d{2}\\n', text)\n",
        "    # Remove the timestamp pattern\n",
        "    cleaned_text = re.sub(r'\\d{2}:\\d{2}\\n', '', text)\n",
        "    return cleaned_text, matches\n",
        "\n",
        "# Apply the function and capture the cleaned text and the matches for 'Text'\n",
        "tn = input(\"Type the index of a text to check (1~100): \")\n",
        "tn = int(tn)\n",
        "n = tn-1\n",
        "cleaned_text_original, timestamp_matches_original = remove_and_report_timestamps(df['Text'][n])\n",
        "\n",
        "# Print the number of occurrences and list each occurrence for 'Text'\n",
        "if timestamp_matches_original:\n",
        "    print(f\"Found {len(timestamp_matches_original)} occurrences of the timestamp pattern in original text:\")\n",
        "    for match in timestamp_matches_original:\n",
        "        print(match.strip())  # .strip() is used to remove any trailing newline for clean display\n",
        "else:\n",
        "    print(\"No timestamp pattern found in the original text.\")\n",
        "\n",
        "# Apply the same function and capture the cleaned text and the matches for 'Cleanedtext01'\n",
        "cleaned_text_cleaned, timestamp_matches_cleaned = remove_and_report_timestamps(df['Cleanedtext01'][n])\n",
        "\n",
        "# Print the number of occurrences and list each occurrence for 'Cleanedtext01'\n",
        "if timestamp_matches_cleaned:\n",
        "    print(f\"Found {len(timestamp_matches_cleaned)} occurrences of the timestamp pattern in cleaned text:\")\n",
        "    for match in timestamp_matches_cleaned:\n",
        "        print(match.strip())\n",
        "else:\n",
        "    print(\"No timestamp pattern found in the cleaned text.\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "1b7_lZARuYHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown 2. Check the second (parenthetical notes) for both 'Text' and 'Cleanedtext01'\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Assuming 'df' is your DataFrame\n",
        "def remove_and_report_timestamps(text):\n",
        "    # Find all occurrences of the timestamp pattern\n",
        "    matches = re.findall(r'\\(.*?\\)', text)\n",
        "    # Remove the timestamp pattern\n",
        "    cleaned_text = re.sub(r'\\(.*?\\)', '', text)\n",
        "    return cleaned_text, matches\n",
        "\n",
        "# Apply the function and capture the cleaned text and the matches for 'Text'\n",
        "ts = input(\"Which text to check (1~100): \")\n",
        "ts = int(ts)\n",
        "s = ts-1\n",
        "cleaned_text_original, timestamp_matches_original = remove_and_report_timestamps(df['Text'][s])\n",
        "\n",
        "# Print the number of occurrences and list each occurrence for 'Text'\n",
        "if timestamp_matches_original:\n",
        "    print(f\"Found {len(timestamp_matches_original)} occurrences of the timestamp pattern in original text:\")\n",
        "    for match in timestamp_matches_original:\n",
        "        print(match.strip())  # .strip() is used to remove any trailing newline for clean display\n",
        "else:\n",
        "    print(\"No timestamp pattern found in the original text.\")\n",
        "\n",
        "# Apply the same function and capture the cleaned text and the matches for 'Cleanedtext01'\n",
        "cleaned_text_cleaned, timestamp_matches_cleaned = remove_and_report_timestamps(df['Cleanedtext01'][s])\n",
        "\n",
        "# Print the number of occurrences and list each occurrence for 'Cleanedtext01'\n",
        "if timestamp_matches_cleaned:\n",
        "    print(f\"Found {len(timestamp_matches_cleaned)} occurrences of the timestamp pattern in cleaned text:\")\n",
        "    for match in timestamp_matches_cleaned:\n",
        "        print(match.strip())\n",
        "else:\n",
        "    print(\"No parenthetical pattern found in the cleaned text.\")\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "4ZYSbiLsuYHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## [4] Text to combine for searching (to check)"
      ],
      "metadata": {
        "id": "yKL9NlRnwQvL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from io import StringIO\n",
        "import string\n",
        "\n",
        "\n",
        "# 2) Combine all items in the 'Text' column as a single string and remove punctuation\n",
        "combined_text = ''.join(df['Cleanedtext01'].astype(str))\n",
        "combined_text = combined_text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "# 3) Get user input for the word to find\n",
        "search_word = input(\"Enter the word to find: \")\n",
        "match_type = input(\"Type 'c' complete matches only, or 'p' for partial matches: \").lower()\n",
        "\n",
        "# 4) Function to find occurrences\n",
        "def find_occurrences(text, word, match_type):\n",
        "    occurrences = []\n",
        "    position = 0\n",
        "    while True:\n",
        "        if match_type == 'c':\n",
        "            # Find whole words only by using boundaries\n",
        "            position = text.lower().find(f' {word.lower()} ', position)\n",
        "        else:\n",
        "            position = text.lower().find(word.lower(), position)\n",
        "\n",
        "        if position == -1:  # No more occurrences found\n",
        "            break\n",
        "        # Calculate start and end positions for slicing\n",
        "        start = max(0, position - 30)\n",
        "        end = min(len(text), position + len(word) + 30)\n",
        "        occurrences.append(text[start:end])\n",
        "        position += len(word)  # Move past this occurrence\n",
        "\n",
        "    return occurrences\n",
        "\n",
        "occurrences = find_occurrences(combined_text, search_word, match_type)\n",
        "\n",
        "# 5) Decide how many occurrences to display\n",
        "print(f\"Total occurrences found: {len(occurrences)}\")\n",
        "print(\"=\"*50)\n",
        "if len(occurrences) > 10:\n",
        "    choice = input(\"More than 10 occurrences found. Type 'a' to display all or '10' to display only the first 10: \").lower()\n",
        "    print(\"=\"*50)\n",
        "    if choice == '10':\n",
        "        occurrences = occurrences[:10]\n",
        "\n",
        "# 6) Display occurrences\n",
        "for occurrence in occurrences:\n",
        "    print(occurrence)\n",
        "\n",
        "# 7) Print summary\n",
        "print(\"=\"*50)\n"
      ],
      "metadata": {
        "id": "ehy6FSAAxd0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving the processed file"
      ],
      "metadata": {
        "id": "sZJOJ_wH7Xm4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head())\n",
        "df.to_csv(\"Cleanedtext01.csv\", encoding = \"utf-8\", index=False)"
      ],
      "metadata": {
        "id": "9KqbYw4Y7cMJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}