{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sundaybest3/s24Corpus-final/blob/main/Corpus/NLTK_spokenwritten.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLTK analysis example: spoken vs. written comparison (0605-updated)"
      ],
      "metadata": {
        "id": "taM6BlTkRraH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "HPiJhcTlhQXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# POS tagging using nltk and finding 'passives' and 'passives with agent'\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XzrjujxNip34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from io import StringIO\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')  # Tokenizer model\n",
        "nltk.download('averaged_perceptron_tagger')  # POS tagger model\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "fljaA3hxcvnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to read and process data from a URL\n",
        "def read_and_process_data(url):\n",
        "    response = requests.get(url)\n",
        "    data = pd.read_csv(StringIO(response.text))\n",
        "    combined_text = ' '.join(data['text'].astype(str))\n",
        "    tokens = word_tokenize(combined_text)\n",
        "    tokens = [word for word in tokens if word.isalpha()]  # Remove punctuation\n",
        "    return tokens\n",
        "\n",
        "# URLs for spoken and written data\n",
        "spoken_url = 'https://raw.githubusercontent.com/sundaybest3/s24Corpus-final/main/Corpus/Cleantext0605.csv'\n",
        "written_url = 'https://raw.githubusercontent.com/sundaybest3/s24Corpus-final/main/now_final.csv'\n",
        "\n",
        "# Process spoken data\n",
        "spoken_tokens = read_and_process_data(spoken_url)\n",
        "\n",
        "# Process written data\n",
        "written_tokens = read_and_process_data(written_url)\n"
      ],
      "metadata": {
        "id": "mErKcma1c37n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to find 'be + past participle' patterns\n",
        "def find_be_pp(tokens):\n",
        "    be_forms = re.compile(r'\\b(am|is|are|get|got|was|were|been|being|getting)\\b', re.IGNORECASE)\n",
        "    past_participle_pattern = re.compile(r'\\b\\w+ed\\b|\\b\\w+n\\b', re.IGNORECASE)\n",
        "    matches = []\n",
        "    tagged_tokens = nltk.pos_tag(tokens)\n",
        "\n",
        "    for i in range(len(tagged_tokens) - 1):\n",
        "        if be_forms.match(tagged_tokens[i][0]):\n",
        "            if past_participle_pattern.match(tagged_tokens[i + 1][0]) and tagged_tokens[i + 1][1] == 'VBN':\n",
        "                matches.append(f\"{tagged_tokens[i][0]} {tagged_tokens[i + 1][0]}\")\n",
        "\n",
        "    return matches\n",
        "\n",
        "# Function to find 'be + past participle + by + agent' patterns\n",
        "def find_be_pp_by_agent(tokens):\n",
        "    be_forms = re.compile(r'\\b(am|is|are|get|got|was|were|been|being|getting)\\b', re.IGNORECASE)\n",
        "    past_participle_pattern = re.compile(r'\\b\\w+ed\\b|\\b\\w+n\\b', re.IGNORECASE)\n",
        "    matches = []\n",
        "    tagged_tokens = nltk.pos_tag(tokens)\n",
        "\n",
        "    for i in range(len(tagged_tokens) - 3):\n",
        "        if be_forms.match(tagged_tokens[i][0]) and past_participle_pattern.match(tagged_tokens[i + 1][0]) and tagged_tokens[i + 1][1] == 'VBN':\n",
        "            if tagged_tokens[i + 2][0].lower() == 'by' and tagged_tokens[i + 3][1] in ['NN', 'NNS', 'NNP', 'NNPS']:\n",
        "                matches.append(f\"{tagged_tokens[i][0]} {tagged_tokens[i + 1][0]} by {tagged_tokens[i + 3][0]}\")\n",
        "\n",
        "    return matches\n",
        "\n",
        "# Find matches in spoken data\n",
        "spoken_be_pp_matches = find_be_pp(spoken_tokens)\n",
        "spoken_be_pp_by_agent_matches = find_be_pp_by_agent(spoken_tokens)\n",
        "\n",
        "# Find matches in written data\n",
        "written_be_pp_matches = find_be_pp(written_tokens)\n",
        "written_be_pp_by_agent_matches = find_be_pp_by_agent(written_tokens)\n",
        "\n",
        "print(spoken_be_pp_matches)\n",
        "print(spoken_be_pp_by_agent_matches)\n",
        "print(written_be_pp_matches)\n",
        "print(written_be_pp_by_agent_matches)\n",
        "\n"
      ],
      "metadata": {
        "id": "94f4YQo2dCWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert lists to sets for comparison\n",
        "spoken_be_pp_set = set(spoken_be_pp_matches)\n",
        "spoken_be_pp_by_agent_set = set(spoken_be_pp_by_agent_matches)\n",
        "written_be_pp_set = set(written_be_pp_matches)\n",
        "written_be_pp_by_agent_set = set(written_be_pp_by_agent_matches)\n",
        "\n",
        "# Find common matches within each dataset\n",
        "common_spoken_matches = spoken_be_pp_by_agent_set.intersection(spoken_be_pp_set)\n",
        "common_written_matches = written_be_pp_by_agent_set.intersection(written_be_pp_set)\n",
        "\n",
        "# Print results\n",
        "print(\"Common matches in spoken data:\")\n",
        "print(common_spoken_matches)\n",
        "\n",
        "print(\"\\nCommon matches in written data:\")\n",
        "print(common_written_matches)"
      ],
      "metadata": {
        "id": "KhEdwbHYAiy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Calculate total occurrences\n",
        "total_occurrences_spoken = len(spoken_be_pp_matches) + len(spoken_be_pp_by_agent_matches)\n",
        "total_occurrences_written = len(written_be_pp_matches) + len(written_be_pp_by_agent_matches)\n",
        "\n",
        "\n",
        "# Create the dataframe\n",
        "data = {\n",
        "    'Category': ['Spoken be+p.p.', 'Spoken be+p.p.+by+agent', 'Written be+p.p.', 'Written be+p.p.+by+agent'],\n",
        "    'Occurrences': [len(spoken_be_pp_matches), len(spoken_be_pp_by_agent_matches), len(written_be_pp_matches), len(written_be_pp_by_agent_matches)],\n",
        "    'Percentage': [\n",
        "        len(spoken_be_pp_matches) / total_occurrences_spoken * 100,\n",
        "        len(spoken_be_pp_by_agent_matches) / total_occurrences_spoken * 100,\n",
        "        len(written_be_pp_matches) / total_occurrences_written * 100,\n",
        "        len(written_be_pp_by_agent_matches) / total_occurrences_written * 100\n",
        "    ]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "print(df)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JwyT7h8iq3JX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Your data\n",
        "data = {\n",
        "    \"Category\": [\"Spoken agentless passives.\", \"Spoken passives with agent\", \"Written agentless passives\", \"Written passives with agent\"],\n",
        "    \"Occurrences\": [620, 24, 9211, 445]\n",
        "}\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Colors for the bars\n",
        "colors = ['skyblue', 'salmon', 'lightgreen', 'orange']\n",
        "\n",
        "# Plotting the data\n",
        "plt.figure(figsize=(10, 6))\n",
        "bars = plt.bar(df[\"Category\"], df[\"Occurrences\"], color=colors)\n",
        "\n",
        "# Adding numbers on top of each bar\n",
        "for bar in bars:\n",
        "    yval = bar.get_height()\n",
        "    plt.text(bar.get_x() + bar.get_width()/2.0, yval, int(yval), va='bottom')  # va: vertical alignment\n",
        "\n",
        "# Adding titles and labels\n",
        "plt.title('Occurrences by Category')\n",
        "plt.xlabel('Category')\n",
        "plt.ylabel('Occurrences')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cjjYvv098XKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# Create the contingency table\n",
        "data = [[620, 9211],\n",
        "        [24,445]]\n",
        "\n",
        "# Conduct the chi-squared test\n",
        "chi2, p, dof, expected = chi2_contingency(data)\n",
        "\n",
        "# Display the results\n",
        "print(\"Chi-squared statistic:\", chi2)\n",
        "print(\"p-value:\", p)\n",
        "print(\"Degrees of freedom:\", dof)\n",
        "print(\"Expected frequencies:\")\n",
        "print(expected)"
      ],
      "metadata": {
        "id": "fv0fTTcT0s6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "The End"
      ],
      "metadata": {
        "id": "frSz-yLWYPst"
      }
    }
  ]
}