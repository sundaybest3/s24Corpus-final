{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sundaybest3/s24Corpus-final/blob/main/Corpus/NLTK_spokenwritten.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLTK analysis example: spoken vs. written comparison (0605-updated)"
      ],
      "metadata": {
        "id": "taM6BlTkRraH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "HPiJhcTlhQXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# POS tagging using nltk and finding 'be+p.p.' and 'be+p.p.+by+agent'\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XzrjujxNip34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from io import StringIO\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')  # Tokenizer model\n",
        "nltk.download('averaged_perceptron_tagger')  # POS tagger model\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "fljaA3hxcvnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to read and process data from a URL\n",
        "def read_and_process_data(url):\n",
        "    response = requests.get(url)\n",
        "    data = pd.read_csv(StringIO(response.text))\n",
        "    combined_text = ' '.join(data['text'].astype(str))\n",
        "    tokens = word_tokenize(combined_text)\n",
        "    tokens = [word for word in tokens if word.isalpha()]  # Remove punctuation\n",
        "    return tokens\n",
        "\n",
        "# URLs for spoken and written data\n",
        "spoken_url = 'https://github.com/sundaybest3/s24Corpus-final/raw/main/Corpus/TEDdata/Cleantext0605.csv'  # Replace with your actual URL for spoken data\n",
        "written_url = 'https://raw.githubusercontent.com/sundaybest3/s24Corpus-final/main/now_final.csv'  # Replace with your actual URL for written data\n",
        "\n",
        "# Process spoken data\n",
        "spoken_tokens = read_and_process_data(spoken_url)\n",
        "\n",
        "# Process written data\n",
        "written_tokens = read_and_process_data(written_url)\n"
      ],
      "metadata": {
        "id": "mErKcma1c37n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to find 'be + past participle' patterns\n",
        "def find_be_pp(tokens):\n",
        "    be_forms = re.compile(r'\\b(am|is|are|get|got|was|were|been|being)\\b', re.IGNORECASE)\n",
        "    past_participle_pattern = re.compile(r'\\b\\w+ed\\b|\\b\\w+n\\b', re.IGNORECASE)\n",
        "    matches = []\n",
        "    tagged_tokens = nltk.pos_tag(tokens)\n",
        "\n",
        "    for i in range(len(tagged_tokens) - 1):\n",
        "        if be_forms.match(tagged_tokens[i][0]):\n",
        "            if past_participle_pattern.match(tagged_tokens[i + 1][0]) and tagged_tokens[i + 1][1] == 'VBN':\n",
        "                matches.append(f\"{tagged_tokens[i][0]} {tagged_tokens[i + 1][0]}\")\n",
        "\n",
        "    return matches\n",
        "\n",
        "# Function to find 'be + past participle + by + agent' patterns\n",
        "def find_be_pp_by_agent(tokens):\n",
        "    be_forms = re.compile(r'\\b(am|is|are|get|got|was|were|been|being)\\b', re.IGNORECASE)\n",
        "    past_participle_pattern = re.compile(r'\\b\\w+ed\\b|\\b\\w+n\\b', re.IGNORECASE)\n",
        "    matches = []\n",
        "    tagged_tokens = nltk.pos_tag(tokens)\n",
        "\n",
        "    for i in range(len(tagged_tokens) - 3):\n",
        "        if be_forms.match(tagged_tokens[i][0]) and past_participle_pattern.match(tagged_tokens[i + 1][0]) and tagged_tokens[i + 1][1] == 'VBN':\n",
        "            if tagged_tokens[i + 2][0].lower() == 'by' and tagged_tokens[i + 3][1] in ['NN', 'NNS', 'NNP', 'NNPS']:\n",
        "                matches.append(f\"{tagged_tokens[i][0]} {tagged_tokens[i + 1][0]} by {tagged_tokens[i + 3][0]}\")\n",
        "\n",
        "    return matches\n",
        "\n",
        "# Find matches in spoken data\n",
        "spoken_be_pp_matches = find_be_pp(spoken_tokens)\n",
        "spoken_be_pp_by_agent_matches = find_be_pp_by_agent(spoken_tokens)\n",
        "\n",
        "# Find matches in written data\n",
        "written_be_pp_matches = find_be_pp(written_tokens)\n",
        "written_be_pp_by_agent_matches = find_be_pp_by_agent(written_tokens)\n",
        "\n",
        "# Calculate total occurrences\n",
        "total_occurrences_spoken = len(spoken_be_pp_matches) + len(spoken_be_pp_by_agent_matches)\n",
        "total_occurrences_written = len(written_be_pp_matches) + len(written_be_pp_by_agent_matches)\n",
        "total_occurrences = total_occurrences_spoken + total_occurrences_written\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "94f4YQo2dCWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataframe\n",
        "data = {\n",
        "    'Category': ['Spoken passives', 'Spoken passives with agent', 'Written passives', 'Written passives with agent'],\n",
        "    'Occurrences': [len(spoken_be_pp_matches), len(spoken_be_pp_by_agent_matches), len(written_be_pp_matches), len(written_be_pp_by_agent_matches)],\n",
        "    'Total Occurrences': [total_occurrences_spoken, total_occurrences_spoken, total_occurrences_written, total_occurrences_written],\n",
        "    'Percentage': [\n",
        "        len(spoken_be_pp_matches) / total_occurrences * 100,\n",
        "        len(spoken_be_pp_by_agent_matches) / total_occurrences * 100,\n",
        "        len(written_be_pp_matches) / total_occurrences * 100,\n",
        "        len(written_be_pp_by_agent_matches) / total_occurrences * 100\n",
        "    ]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Display the dataframe\n",
        "print(df)"
      ],
      "metadata": {
        "id": "d5biO25uMAvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the dataframe with reordered categories\n",
        "data = {\n",
        "    'Category': ['Spoken passives', 'Written passives', 'Spoken passives with agent', 'Written passives with agent'],\n",
        "    'Occurrences': [620, 9211, 24, 445],\n",
        "    'Total Occurrences': [644, 9656, 644, 9656],\n",
        "    'Percentage': [6.019417, 89.427184, 0.233010, 4.320388]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Visualization\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "bars = ax.bar(df['Category'], df['Occurrences'], color=['blue', 'orange', 'green', 'red'])\n",
        "\n",
        "# Annotate each bar with the value\n",
        "for bar in bars:\n",
        "    yval = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2, yval, int(yval), ha='center', va='bottom')\n",
        "\n",
        "plt.xlabel('Category')\n",
        "plt.ylabel('Occurrences')\n",
        "plt.title('Occurrences of Passives and Passives with Agent in Spoken and Written Text')\n",
        "\n",
        "# Save the plot as a file\n",
        "plt.savefig('Occurrences_comparison.png')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cjjYvv098XKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the dataframe\n",
        "data = {\n",
        "    'Category': ['Spoken passives', 'Spoken passives with agent', 'Written passives', 'Written passives with agent'],\n",
        "    'Occurrences': [620, 24, 9211, 445],\n",
        "    'Total Occurrences': [644, 644, 9656, 9656],\n",
        "    'Percentage': [6.019417, 0.233010, 89.427184, 4.320388]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Pie chart\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "ax.pie(df['Percentage'], labels=df['Category'], autopct='%1.1f%%', startangle=90, colors=['blue', 'green', 'orange', 'red'])\n",
        "\n",
        "plt.title('Percentage of Passives and Passives with Agent in Spoken and Written Text')\n",
        "\n",
        "# Save the plot as a file\n",
        "plt.savefig('Percentage_comparison.png')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NjOZM2acNpW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.stats.proportion import proportions_ztest\n",
        "\n",
        "# Counts of passives without agent\n",
        "count = np.array([620, 9211])\n",
        "nobs = np.array([644, 9656])\n",
        "\n",
        "# Perform the two-proportion z-test\n",
        "stat, pval = proportions_ztest(count, nobs)\n",
        "\n",
        "print(f\"Z-statistic: {stat}\")\n",
        "print(f\"p-value: {pval}\")"
      ],
      "metadata": {
        "id": "-BvdPbYwjU9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Counts of passives with agent\n",
        "count_with_agent = np.array([24, 445])\n",
        "nobs_with_agent = np.array([644, 9656])\n",
        "\n",
        "# Perform the two-proportion z-test\n",
        "stat_with_agent, pval_with_agent = proportions_ztest(count_with_agent, nobs_with_agent)\n",
        "\n",
        "print(f\"Z-statistic (with agent): {stat_with_agent}\")\n",
        "print(f\"p-value (with agent): {pval_with_agent}\")"
      ],
      "metadata": {
        "id": "WplH_3j-j0tF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Data for visualization\n",
        "categories = ['Spoken passives', 'Written passives', 'Spoken passives with agent', 'Written passives with agent']\n",
        "proportions = [620/644, 9211/9656, 24/644, 445/9656]\n",
        "\n",
        "# Create a bar plot\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "bars = ax.bar(categories, proportions, color=['blue', 'orange', 'green', 'red'])\n",
        "\n",
        "# Annotate each bar with the value\n",
        "for bar in bars:\n",
        "    yval = bar.get_height()\n",
        "    ax.text(bar.get_x() + bar.get_width()/2, yval, f'{yval:.2%}', ha='center', va='bottom')\n",
        "\n",
        "plt.xlabel('Category')\n",
        "plt.ylabel('Proportion')\n",
        "plt.title('Proportions of Passives and Passives with Agent in Spoken and Written Texts')\n",
        "\n",
        "# Save the plot as a file\n",
        "plt.tight_layout()\n",
        "plt.savefig('Proportions_comparison.png')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kKK1BnhikP7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# Create the contingency table\n",
        "data = np.array([[620, 24],\n",
        "                 [9211, 445]])\n",
        "\n",
        "# Perform the chi-square test\n",
        "chi2, p, dof, expected = chi2_contingency(data)\n",
        "\n",
        "# Output the results\n",
        "print(\"Chi-square statistic:\", chi2)\n",
        "print(\"p-value:\", p)\n",
        "print(\"Degrees of freedom:\", dof)\n",
        "print(\"Expected frequencies:\\n\", expected)\n",
        "\n"
      ],
      "metadata": {
        "id": "D7osBKNFHVVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "The End"
      ],
      "metadata": {
        "id": "frSz-yLWYPst"
      }
    }
  ]
}