{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sundaybest3/s24Corpus-final/blob/main/Corpus/NLTK_spokenwritten.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLTK analysis example: spoken vs. written comparison (0605-updated)"
      ],
      "metadata": {
        "id": "taM6BlTkRraH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "HPiJhcTlhQXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# POS tagging using nltk and finding 'be+p.p.' and 'be+p.p.+by+agent'\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XzrjujxNip34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from io import StringIO\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')  # Tokenizer model\n",
        "nltk.download('averaged_perceptron_tagger')  # POS tagger model"
      ],
      "metadata": {
        "id": "fljaA3hxcvnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to read and process data from a URL\n",
        "def read_and_process_data(url):\n",
        "    response = requests.get(url)\n",
        "    data = pd.read_csv(StringIO(response.text))\n",
        "    combined_text = ' '.join(data['text'].astype(str))\n",
        "    tokens = word_tokenize(combined_text)\n",
        "    tokens = [word for word in tokens if word.isalpha()]  # Remove punctuation\n",
        "    return tokens\n",
        "\n",
        "# URLs for spoken and written data\n",
        "spoken_url = 'https://github.com/sundaybest3/s24Corpus-final/raw/main/Corpus/TEDdata/Cleantext0605.csv'  # Replace with your actual URL for spoken data\n",
        "written_url = 'https://raw.githubusercontent.com/sundaybest3/s24Corpus-final/main/now_final.csv'  # Replace with your actual URL for written data\n",
        "\n",
        "# Process spoken data\n",
        "spoken_tokens = read_and_process_data(spoken_url)\n",
        "\n",
        "# Process written data\n",
        "written_tokens = read_and_process_data(written_url)\n"
      ],
      "metadata": {
        "id": "mErKcma1c37n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to find 'be + past participle' patterns\n",
        "def find_be_pp(tokens):\n",
        "    be_forms = re.compile(r'\\b(am|is|are|was|were|been|being)\\b', re.IGNORECASE)\n",
        "    past_participle_pattern = re.compile(r'\\b\\w+ed\\b|\\b\\w+n\\b', re.IGNORECASE)\n",
        "    matches = []\n",
        "    tagged_tokens = nltk.pos_tag(tokens)\n",
        "\n",
        "    for i in range(len(tagged_tokens) - 1):\n",
        "        if be_forms.match(tagged_tokens[i][0]):\n",
        "            if past_participle_pattern.match(tagged_tokens[i + 1][0]) and tagged_tokens[i + 1][1] == 'VBN':\n",
        "                matches.append(f\"{tagged_tokens[i][0]} {tagged_tokens[i + 1][0]}\")\n",
        "\n",
        "    return matches\n",
        "\n",
        "# Function to find 'be + past participle + by + agent' patterns\n",
        "def find_be_pp_by_agent(tokens):\n",
        "    be_forms = re.compile(r'\\b(am|is|are|was|were|been|being)\\b', re.IGNORECASE)\n",
        "    past_participle_pattern = re.compile(r'\\b\\w+ed\\b|\\b\\w+n\\b', re.IGNORECASE)\n",
        "    matches = []\n",
        "    tagged_tokens = nltk.pos_tag(tokens)\n",
        "\n",
        "    for i in range(len(tagged_tokens) - 3):\n",
        "        if be_forms.match(tagged_tokens[i][0]) and past_participle_pattern.match(tagged_tokens[i + 1][0]) and tagged_tokens[i + 1][1] == 'VBN':\n",
        "            if tagged_tokens[i + 2][0].lower() == 'by' and tagged_tokens[i + 3][1] in ['NN', 'NNS', 'NNP', 'NNPS']:\n",
        "                matches.append(f\"{tagged_tokens[i][0]} {tagged_tokens[i + 1][0]} by {tagged_tokens[i + 3][0]}\")\n",
        "\n",
        "    return matches\n",
        "\n",
        "# Find matches in spoken data\n",
        "spoken_be_pp_matches = find_be_pp(spoken_tokens)\n",
        "spoken_be_pp_by_agent_matches = find_be_pp_by_agent(spoken_tokens)\n",
        "\n",
        "# Find matches in written data\n",
        "written_be_pp_matches = find_be_pp(written_tokens)\n",
        "written_be_pp_by_agent_matches = find_be_pp_by_agent(written_tokens)\n",
        "\n",
        "# Calculate total occurrences\n",
        "total_occurrences = sum([\n",
        "    len(spoken_be_pp_matches), len(spoken_be_pp_by_agent_matches),\n",
        "    len(written_be_pp_matches), len(written_be_pp_by_agent_matches)\n",
        "])\n"
      ],
      "metadata": {
        "id": "94f4YQo2dCWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CKTe6mJWa0E",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create a DataFrame to display the results and percentages\n",
        "data = {\n",
        "    'Pattern': ['be + past participle', 'be + past participle + by + agent'],\n",
        "    'Spoken Occurrences': [len(spoken_be_pp_matches), len(spoken_be_pp_by_agent_matches)],\n",
        "    'Written Occurrences': [len(written_be_pp_matches), len(written_be_pp_by_agent_matches)]\n",
        "}\n",
        "\n",
        "df_occurrences = pd.DataFrame(data)\n",
        "df_occurrences['Total Occurrences'] = df_occurrences['Spoken Occurrences'] + df_occurrences['Written Occurrences']\n",
        "df_occurrences['Percentage'] = (df_occurrences['Total Occurrences'] / total_occurrences) * 100\n",
        "\n",
        "print(df_occurrences)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Visualization\n",
        "labels = ['Spoken be+p.p.', 'Spoken be+p.p.+by+agent', 'Written be+p.p.', 'Written be+p.p.+by+agent']\n",
        "values = [len(spoken_be_pp_matches), len(spoken_be_pp_by_agent_matches), len(written_be_pp_matches), len(written_be_pp_by_agent_matches)]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(labels, values, color=['blue', 'green', 'orange', 'red'])\n",
        "plt.xlabel('Patterns and Data Types')\n",
        "plt.ylabel('Number of Occurrences')\n",
        "plt.title('Occurrences of \"be + past participle\" and \"be + past participle + by + agent\"')\n",
        "plt.show()\n",
        "\n",
        "plt.savefig('Occurrences_comparison.png')  # Save the plot as a file"
      ],
      "metadata": {
        "id": "cjjYvv098XKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chi-Squared Test\n",
        "observed = [\n",
        "    [len(spoken_be_pp_matches), len(spoken_be_pp_by_agent_matches)],\n",
        "    [len(written_be_pp_matches), len(written_be_pp_by_agent_matches)]\n",
        "]\n",
        "\n",
        "chi2_stat, p_val, dof, ex = chi2_contingency(observed)\n",
        "\n",
        "print(f\"\\nChi-squared Test:\")\n",
        "print(f\"Chi-squared Statistic: {chi2_stat}\")\n",
        "print(f\"p-value: {p_val}\")\n",
        "print(f\"Degrees of Freedom: {dof}\")\n",
        "print(f\"Expected Frequencies: {ex}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "D7osBKNFHVVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "The End"
      ],
      "metadata": {
        "id": "frSz-yLWYPst"
      }
    }
  ]
}